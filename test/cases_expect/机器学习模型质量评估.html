<html><head></head><body><div><h1>机器学习模型质量评估</h1><p>团队同学的一篇文章，总结了我们观远在模型测试评估，可观测性等方面的实践，与大家一起分享交流。</p><h2>Introduction</h2><p>ML Model Testing Framework 可以理解为测试机器学习模型能力的框架，简单称为模型质量评估。</p><p><b>什么是模型质量评估</b></p><p>在将模型正式上线之前，通常的做法是通过回测来评估模型的准确性和可靠性。然而，仅仅在回测数据集上的表现优异并不能保证模型在上线后的稳定性和持续性。因此，在模型质量评估中，我们需要从更广泛的角度考虑整个模型流程。这意味着我们需要建立一套用于对整个模型流程进行评估和测试的框架，以确保模型在上线后能够保持准确、可靠和健壮的表现。</p><p><b>为什么要做模型质量评估</b></p><p>实际业务场景的复杂性和多变性使得仅基于当前的业务知识和行业背景构建的流程可能无法应对未来不断变化的情况。因此，在模型构建过程中，我们需要为各个重要环节设置检查关卡，及时发现并解决问题。这样可以降低数据科学家排查问题的难度和时间，从而保障模型上线后的交付质量，提高整个模型流程在实际应用中的可靠性。具体来说，建立一套用于对整个模型流程质量评估的框架，可以保障模型质量、提高开发效率、确保模型安全性且提升模型可维护性。</p><p><b>怎么做模型质量评估</b></p><p>总体来说，可以分为 Data（数据检测）、Features（特征检测）、ML Algorithms（算法模型检测）三个组件，每个组件都需要进行相应的检查，具体的检查要点如下图所示，下文将对每个组件的检查要点进行详细介绍。</p><figure><img src="https://pic4.zhimg.com/v2-7c4e0d9b995979b5e3dc65b4225be0e7_b.jpg"></figure><h2>Components</h2><h3>Data</h3><p><b>为什么要做</b></p><p>数据是模型成功之母。如果无法为模型提供良好的输入数据，即使是最先进的算法也无法产生出色的应用结果。因此，在检查过程中需要确保数据是否符合建模条件。</p><p><b>怎么做</b></p><p><b>Incorrect Labels/Values</b></p><p>这个检查指在找出标记的值和实际不符的数据。可以根据数据本身、常识或业务逻辑来判断是否存在错误值。例如，在预测某地空气温度时，如果训练数据中包含 1000 摄氏度这种明显与基本常识不符的数值，应考虑对数据进行修正。如果无法修正，应将其视为错误数据，并进行样本剔除或置为空值处理，以便按照处理后的数据进行后续操作。</p><p><b>Data Value Detection</b></p><p>这个检查是指找出缺失、异常的数据。在数据处理和异常检测中，有许多常用的算法可以用于检查缺失值、异常值和离群值。以下是一些常见的算法：</p><p><b>缺失值</b></p><p>检查：</p><ul><li>可以直接用 isnull 来提取空置，也可以使用 <a href="https://link.zhihu.com/?target=https%3A//deepchecks.com/">DeepChecks</a>（DeepChecks 是一个用于数据质量评估和数据预处理的 Python 包，它包含但不限于检查缺失值、异常值、重复值和新出现的类别值等常见数据质量问题的方法）中的 Percent Of Nulls、Mixed Nulls 方法完成。</li></ul><p>处理：</p><ul><li>利用统计指标，如均值、中位数、众数等来填充或删除缺失值。</li><li>使用基于机器学习的方法，如 KNN（K 近邻算法）、回归、随机森林等来预测缺失值。</li></ul><p><b>异常值</b></p><p>检查：</p><ul><li>基于统计学的方法，如 Z-score、箱线图、3σ原则等来识别数据中的异常值。</li><li>基于距离或相似性的方法，如 KNN、LOF（局部离群因子）等来检测数据点的离群程度。</li><li>基于聚类的方法，如 K-means、DBSCAN（基于密度的聚类算法）等来将离群值划分到独立的簇中。</li><li>基于机器学习的方法，如 SVM（支持向量机）、随机森林、神经网络等来识别异常值。</li></ul><p>处理：</p><ul><li>删除：可以直接删除包含异常值的样本或特征。这种方法适用于异常值对模型的影响较小且数量较少的情况，但需要慎重使用，因为删除数据可能会导致信息丢失。</li><li>替代：可以使用一些统计量（如均值、中位数、众数等）或插值方法（如线性插值、多项式插值等）来替代异常值。这种方法可以在保留数据的同时修复异常值，但可能会引入一定的误差。</li><li>离散化处理：可以将连续型特征离散化为多个离散的取值，将异常值映射到最近的离散值上。这种方法适用于异常值对离散特征的影响较小的情况。</li></ul><p><b>Data Drift</b></p><p>数据漂移，是指数据分布随着时间发生了变化，这有可能导致模型在实际应用中的性能下降。</p><p>检查：</p><ul><li>监控数据分布：定期对模型输入数据的分布进行监控，包括特征的统计特性（如均值、方差、分位数等）和数据的类别分布等。通过比较当前数据分布与训练数据分布的差异，可以判断是否发生了数据漂移。</li><li>统计假设检验：使用统计学中的假设检验方法，比如卡方检验、KS 检验、t 检验等，对模型输入数据在不同时间段或不同环境下的分布进行比较，从而判断是否存在显著的差异，表明可能发生了数据漂移。</li><li>时序分析：对模型输入数据进行时序分析，检测数据分布随时间的变化趋势，如通过绘制时间序列图、移动平均图、趋势图等，观察数据分布的演变情况，从而判断是否发生了数据漂移。</li><li>机器学习模型：这种方法也叫对抗验证，即使用一个监督学习或无监督学习的模型，通过训练这个模型来对训练数据和测试数据进行预测，并通过比较预测结果与实际标签的差异来判断是否发生了数据漂移。</li><li>DeepChecks 中也有方法可以直接使用（<code>CategoryMismatchTrainTest</code>、<code>NewLabelTrainTest</code>、<code>MultivariateDrift</code> 等）。</li></ul><p>处理：</p><ul><li>如果发生了数据漂移或者训练数据非常庞大，需要找出有价值的/贴近真实预测样本的训练样本来训练模型。</li></ul><p>挑选样本的方法如下：</p><ul><li>对抗验证：使用对抗验证模型预测训练集中每个样本为预测集样本的概率，挑选概率比较高即更贴近真实预测集的样本来进行模型训练。</li><li>Data Valuation using Reinforcement Learning：DVRL 基于使用强化学习来评估数据价值，选择最有价值的样本来进行模型训练。这是一种新颖的数据评估元学习框架，它可以输出单个样本用于模型训练的 probability，通过 probability 的排序来挑选有价值的样本纳入到训练当中。 </li><li>Data Shapley：这是一种用于衡量多个数据点对于机器学习模型预测的贡献的方法，可以评估不同数据点对于模型的贡献，从而识别出对于模型最有价值的数据点。</li><li>Leave One Out：LOO 一般广泛用来找到单个数据点的价值。该方法中，主要通过从训练数据集中去除单个数据点，并通过对比移除前后的模型表现，用模型表现差异来表示数据点的价值。但是这个方法计算成本非常高，无法对大量数据中的每个数据点或单个数据重新训练和重新评估。</li></ul><h3>Features</h3><p><b>为什么要做</b></p><p>在机器学习中，特征对模型的性能起到了决定性的作用。只有使用高质量的特征，才能得到高质量的模型表现。</p><p><b>怎么做</b></p><p><b>Feature Thresholds</b></p><p>在进行特征工程时，可以基于常识或者业务背景对特征值设置限定条件，从而限制其取值范围。对于连续型特征，可以设定期望阈值作为数值的上下限。例如，对于人类的年龄特征，我们可以期望其取值在 0 到 100 岁之间，超出这个范围的特征值应被标记为异常值，并进行预警处理。对于类别型特征，限定条件可以是一个枚举值的集合。例如，对于人类的性别特征，我们可以预设只有男、女和未知三种取值，若出现了其他类别的值，则需要进行异常值预警。这样的限定条件有助于在特征工程过程中保持数据的合理性和准确性，从而提高模型的可靠性。这一步跟上述的数据检测中的 Incorrect Labels/Values 不同之处在于检测出特征构建时代码/计算错误引入的不合理特征值问题。</p><p><b>Feature Relevance</b></p><p>特征相关性是衡量不同特征对目标变量（或输出变量）影响程度的指标。计算特征相关性的方法多种多样，包括统计方法（如相关系数、卡方检验、方差分析等）、信息论方法（如互信息、信息增益、基尼系数等）、模型内部的特征重要性评估（如决策树中的特征重要性、线性回归中的系数等）以及基于机器学习算法的特征选择方法（如基于树模型的特征选择、正则化方法等）。在实际应用中，对于经过模型验证或业务认可的重要特征，在后续任务中应该仍然具有高度的模型重要性。例如，在销量预测任务中，历史的 lag 销量可能是最重要的特征之一。因此，在每次的模型训练中，这类特征应该保持较高的贡献度。如果特征的贡献度突然大幅度下降，可能是由于数据相关的问题导致关键特征丢失，比如数据处理逻辑问题或者是数据本身发生了概念漂移。因此，长期来看，应该对特征和目标变量的相关性或特征重要性进行监控。</p><p><b>Feature Relationship</b></p><p>在机器学习建模过程中，对特征之间的相互关系或者相互作用进行检查和探究的过程。通过特征关系检查，可以深入了解特征之间的线性关系、非线性关系、交互作用、因果关系等，并对其进行统计分析和可视化展示，以便更好地理解数据和构建更准确、可解释的机器学习模型。</p><p>在特征关系检查中，可以使用多种方法和技术，例如散点图、线性回归、相关系数、特征组合、交叉验证等。通过对特征之间的关系进行检查，可以帮助我们发现数据中的模式和规律，优化特征工程过程，选择合适的特征表示方式，并对模型的性能和解释性进行改进。特征关系检查对于构建高性能和解释性强的机器学习模型非常重要，能够帮助我们更好地理解数据的特性，挖掘隐藏的信息，提高模型的预测准确性和解释能力。</p><p><b>Feature Leakage</b></p><p>特征泄漏可以使得模型在验证集上表现优异，但是实际落地中表现远远低于预期。例如，在预测第二天是否下雨时，不应使用第二天的空气湿度作为特征，在构造训练集/验证集时，这些数据可以获取到，但是实际落地当中，不可能收集到未来数据。所以在构造特征时，应避免使用与预测目标存在因果关系的未来数据或者预测目标的衍生数据，合理的特征选择和特征关系检查对于构建高效的机器学习模型非常关键。可以使用以下方法进行检查：</p><ul><li>特征重要性分析：检查特征重要性是否在合理范围内，可以使用特征贡献占比（比如 LGBM 模型可以通过 Feature Importance 来计算各个特征的 information gain 来计算占比，或通过特征的 sharply value 来计算占比）检查是否有比重过大的特征，如果头部的特征贡献比重超过一定阈值（可以根据情况设定）可以判定为有信息泄露的可能。</li><li>皮尔森相关系数：检查特征与预测目标之间的相关系数是否在合理范围内。</li><li>特征与预测目标的相关性检查：检查特征与预测目标之间的相关性是否过高，可以使用 DeepChecks 中提供的 <code>FeatureLabelCorrelation</code> 方法进行检查。</li></ul><p>以上方法可以帮助确保在构造特征时不使用不合适的数据，从而提高模型的可靠性和预测性能。</p><p><b>Feature Suitability/Cost</b></p><p>在评估特征的合适性时，可以通过考虑特征的构造成本来进行衡量。通过对特征的构造成本进行评估，可以更好地了解特征生成/提取的可行性和效率，从而优化特征工程过程，提高模型的性能和效果。在实际的机器学习项目中，合理评估特征构造成本对于资源管理和模型性能的优化非常重要。</p><p>以下是一些用于计算特征生成/提取成本的方法：</p><ul><li>内存使用量：考虑生成/提取特征时所需的内存使用量。</li><li>上游数据依赖性：考虑生成/提取特征时是否需要依赖其他数据，以及这些数据的可用性和可访问性。</li><li>推断延迟：考虑生成/提取特征时对推断性能的影响，包括生成/提取特征所需的时间和计算资源。</li></ul><p><b>Feature Compliance</b></p><p>测试特征的合规性是一项关键任务，旨在确保特征（包括原始的和派生的特征）在使用过程中不会违反实际业务场景中的数据合规性要求。这对于确保模型的合法性、合规性和可靠性至关重要。</p><p>例如，考虑到与客户的商业协议中可能存在的限制，比如规定其每月发票不能用于任何分析目的，那么在特征工程过程中，需要将避免使用这些发票数据作为特征。这可以作为测试和质量控制检查的一部分，以确保在模型训练和应用过程中不会违反相关合规性要求。</p><p>为了测试特征合规性，可以考虑以下一些细节和步骤：</p><ul><li>仔细审查与业务场景和合同规定相关的数据使用限制，包括商业协议、法律法规、隐私政策等，以了解特征使用的合规性要求。</li><li>确保在特征工程过程中遵循数据使用限制，避免使用受限制的数据或将其纳入特征构建过程。</li><li>设计和实施合适的测试和质量控制机制，包括自动化检查和人工审查，以确保特征的合规性。</li><li>记录特征合规性检查的结果，并在需要时进行修复和验证，以确保模型在应用阶段仍然符合合规性要求。</li></ul><p><b>Feature Unit Testing</b></p><p>在进行特征生成或更新时，将调整过的代码进行单元测试作为常规开发流程的一部分是一种良好的实践。例如，在构造统计特征时，可以使用简单的测试数据进行统计结果的对比，以确保聚合和统计特征的计算逻辑是正确的。也可以通过编写自动化测试用例来验证特征生成和更新的正确性，可以有效减少潜在的错误和缺陷，并提高代码质量。这可以包括以下几个步骤：</p><ul><li>定义测试用例：根据特征生成和更新的不同场景，定义一系列测试用例，包括不同类型的输入数据、边界值和异常情况。</li><li>编写测试代码：使用合适的测试框架和工具，编写测试代码来执行定义的测试用例，并验证特征生成和更新的计算逻辑是否符合预期。</li><li>自动化测试：将测试用例集成到自动化测试框架中，并将其包含在持续集成和持续交付（CI/CD）流程中，以确保在每次代码提交或部署时都会自动运行测试。</li><li>结果验证：在测试运行完成后，对测试结果进行验证和分析，确保特征生成和更新的计算逻辑是正确的，同时修复任何出现的错误或缺陷。</li></ul><p><b>Feature Static Review</b></p><p>在生成特征的过程中，进行静态代码审查是一种重要的实践，可以通过人工和自动两种方式进行。人工审查应该由除了代码负责人之外的其他人员进行，以确保代码审查的客观性和全面性。同时，还可以借助自动的静态代码分析工具，例如 SonarQube/SonarLint 等，来评估生成的特征代码的质量。</p><p>在进行静态代码审查时，可以考虑以下几个方面：</p><ul><li>代码规范：检查生成的特征代码是否符合项目的代码规范和编码标准，包括命名规范、缩进风格、注释规范等。</li><li>可维护性：评估生成的特征代码的可维护性，包括代码的可读性、可理解性和可扩展性，以确保特征的后续维护和更新工作可以顺利进行。</li><li>效率和性能：检查生成的特征代码是否存在效率和性能上的问题，例如不必要的循环、重复计算等，以确保特征生成的效率和性能是可接受的。</li><li>错误处理：评估生成的特征代码是否有足够的错误处理机制，包括异常处理、错误提示等，以保证特征生成在面对异常情况时能够正确处理。</li></ul><h3>ML Algorithms/Model</h3><p><b>为什么要做</b></p><p>在进行模型部署或交付之前，需要确保模型达到了上线标准，即具备了高质量的性能和可靠的预测能力。为了确保模型的性能符合预期，可以采用一系列方式和方法来进行论证和验证。</p><p><b>怎么做</b></p><p><b>Overfitting/Underfitting</b></p><p>判断模型泛化能力时，需要检查模型是否发生了过拟合或者欠拟合，这可以通过以下方法来实现：</p><ul><li>使用学习曲线（Learning Curve）来判断模型的过拟合和欠拟合情况。学习曲线可以通过绘制不同训练集大小下的训练集和交叉验证集的准确率来观察模型在新数据上的表现，从而判断模型是否存在高方差（过拟合）或高偏差（欠拟合）的情况，并考虑是否需要增加训练数据来减小过拟合现象。</li><li>可以使用 DeepChecks 中的 <code>BoostingOverfit</code> 和 <code>TrainTestPerformance</code> 等方法，或者使用 <code>LightGBMRegressorOverfit</code> 和 <code>LightGBMClassifierOverfit</code> 等方法来判断 LightGBM 模型的树节点叶子数量是否高于数据集大小，以及叶子节点上的纯度是否过高，从而判断模型是否存在过拟合现象。</li><li>进行误差分布分析，将模型误差分解为方差、偏差和不可避免的随机误差三部分。欠拟合通常对应偏差高，即训练数据的拟合能力较弱，导致训练误差较高，此时可以考虑加入更多特征或者使用更复杂的模型进行处理。过拟合通常对应方差高，即模型对训练数据过于敏感，对不同的训练数据效果波动较大，泛化能力较弱，此时可以考虑增加训练数据、降低模型复杂度或者引入正则化项等方法进行处理。也可以使用 DeepChecks 中的 <code>RegressionErrorDistribution</code> 和 <code>RegressionSystematicError</code> 等方法，对模型的回归误差分布和系统性误差进行分析，从而判断模型是否存在过拟合或者欠拟合现象。</li></ul><p><b>Model Performance</b></p><p>对模型的表现进行评估，包括稳定性和合理性的考量，是为了确保模型在实际应用中能够具备稳定的性能，并且生成合理的预测结果。这是因为模型在上线后将直接应用于实际业务场景中，可能会对用户或业务产生重要影响，因此需要对模型的性能进行充分的评估，以减少潜在的风险和不良影响。</p><ul><li>模型的稳定性：通过观察模型在多个随机或业务场景切分的数据集上的表现，检查模型在不同数据集上的预测结果是否波动较小。如果模型在不同数据集上的表现差异较大，可能需要重新调整模型策略以获得相对稳定的性能表现。可以设定稳定性的评估指标，例如预测结果的方差或标准差，以判断模型的稳定性是否符合上线标准。</li><li>模型的合理性：根据对该模型、场景或行业的理解，可以设定一个评价指标的上下限，作为模型性能的合理范围。可以将模型在测试数据集上的表现与这个评价指标进行比较，判断模型是否在合理的范围内。如果模型的表现低于下限或超过上限，可能需要进一步优化或改进模型。</li></ul><p><b>Simple Model Comparison</b></p><p>在进行模型评估时，通过与简单模型的进行对比，客观地评估当前模型水平。这可以帮助开发人员了解模型在实际应用中的表现，并采取相应的优化措施，以持续提升模型的性能和效果。这可以通过使用 DeepChecks 中的 <code>SimpleModelComparison</code> 和 <code>SimpleMovingAverageComparison</code> 方法来实现。</p><p><code>SimpleModelComparison</code> 方法可以将当前模型与一个简单模型进行对比，例如线性模型或者基准模型。可以比较模型在不同性能指标上的表现，例如准确率、精确度、召回率等，从而评估当前模型是否有显著的改进。</p><p><code>SimpleMovingAverageComparison</code> 方法则可以通过计算模型在一段时间内的移动平均性能指标，与简单模型进行对比。这可以帮助检测模型在不同时间段内的性能趋势，判断模型是否趋于稳定，并与简单模型进行比较，从而评估模型的表现。</p><p><b>Prediction Distribution</b></p><p>在模型评估过程中，检查模型预测结果的分布是否与训练集分布一致，是一种重要的评估方法。这可以通过使用 DeepChecks 中的 <code>TrainTestPredictionDrift</code> 方法来实现。</p><p><code>TrainTestPredictionDrift</code> 方法可以帮助我们比较模型在训练集和测试集上的预测结果分布。它可以计算模型在训练集和测试集上的预测分布之间的差异，例如通过计算概率分布、预测值分布等指标来量化差异。通过比较这些指标，可以判断模型在测试集上的预测结果是否与训练集分布一致。</p><p>这种对比可以帮助我们检测模型是否受到数据漂移的影响，即模型在训练集上训练得到的知识是否能够泛化到测试集上。如果模型在测试集上的预测结果分布与训练集存在显著差异，可能意味着模型在面对新数据时的表现可能不如预期。这时，开发人员可以采取相应的调优措施，例如重新训练模型、调整模型参数或者处理数据偏移等，以提高模型的泛化能力和性能。</p><p><b>Model Efficiency</b></p><p>在模型评估过程中，检查模型的性能和训练时间是否合理是非常重要的。可以通过使用 DeepChecks 中的 <code>ModelInferenceTime</code> 方法来实现。</p><p><code>ModelInferenceTime</code> 方法可以帮助我们评估模型在推理阶段的运行时间，即模型从输入到输出的推理时间。这可以通过对模型进行推理测试，并记录推理时间来实现。通过比较不同模型的推理时间，可以判断模型是否在合理的时间内完成推理，并且是否满足业务需求。</p><p>模型训练时间过长可能会导致部署时的性能问题，例如响应时间过长、资源利用率低下等。因此，检查模型的训练时间是否过长，以及模型在推理阶段的性能是否符合预期，对于保证模型在生产环境中的可用性和稳定性至关重要。如果模型的训练时间过长或者推理时间超过了业务需求的限制，开发人员可以考虑优化模型的结构、算法、硬件设备等，以提高模型的性能和效率。</p><h2>Conclusion</h2><p>总之，建立一套用于对整个模型流程进行评估和测试的框架可以保障模型质量、提高开发效率、确保模型安全性，以及提升模型的可维护性，从而在生产环境中获得更好的性能和效果。</p></div></body></html>