<html><head></head><body><div><h1>吴恩达来信：超越测试集——prompting改变了机器学习</h1><p>Dear friends,</p><p>A few weeks ago, I <a href="https://link.zhihu.com/?target=https%3A//www.deeplearning.ai/the-batch/visual-prompting-builds-vision-models-in-seconds/%3Fref%3Ddl-staging-website.ghost.io">wrote</a> about my team at Landing AI’s work on visual prompting. With the speed of building machine learning applications through text prompting and visual prompting, I’m seeing a trend toward building and deploying models without using a test set. This is part of an important trend of speeding up getting models into production.</p><p>The test set has always been a sacred aspect of machine learning development. In academic machine learning work, test sets are the cornerstone of algorithm benchmarking and publishing scientific conclusions. Test sets are also used in commercial machine learning applications to measure and improve performance and to ensure accuracy before and after deployment.</p><p>But thanks to prompt-based development, in which you can build a model simply by providing a text prompt (such as “classify the following text as having either a positive or negative sentiment”) or a visual prompt (by labeling a handful of pixels to show the model what object you want to classify), it is possible to build a decent machine learning model with very few examples (few-shot learning) or no examples at all (zero-shot learning).</p><p>Previously, if we needed 10,000 labeled training examples, then the additional cost of collecting 1,000 test examples didn’t seem onerous. But the rise of zero-shot and few-shot learning — driven by prompt-based development — is making test set collection a bottleneck.</p><p>Thus I'm seeing more and more teams use a process for development and deployment that looks like this:</p><ul><li>Use prompting to develop a model. This can take minutes to hours.</li><li>Deploy the model to production and run it on live data quickly but safely, perhaps by running in “shadow mode,” where the model’s inferences are stored and monitored but not yet used. (More on this below.)</li><li>If the model’s performance is acceptable, let it start making real decisions.</li><li>Only after the model is in production, and only if we need to benchmark more carefully (say, to eke out a few percentage points of performance improvement), collect test data to create a more careful benchmark for further experimentation and development. But if the system is doing well enough, don’t bother with this.</li></ul><p>I’m excited by this process, which significantly shortens the time it takes to build and deploy machine learning models. However, there is one important caveat: In certain applications, a test set is important for managing risk of harm. Many deployments don’t pose a significant risk of harm; for example, a visual inspection system in a smartphone factory that initially shadows a human inspector and whose outputs aren’t used directly yet. But if we're developing a system that will be involved in decisions about healthcare, criminal justice, finance, insurance, and so on, where inaccurate outputs or bias could cause significant harm, then it remains important to collect a rigorous test set and deeply validate the model’s performance before allowing it to make consequential decisions.</p><p>The occurrence of concept drift and data drift can make the very notion of a “test set” problematic in practical applications, because the data saved for testing no longer matches the real distribution of input data. For this reason, the best test data is production data. For applications where it’s safe and reasonable to deploy without using a test set, I’m excited about how this can speed up development and deployment of machine learning applications.</p><p>Keep learning!</p><p>Andrew</p><hr><p>几周前，我撰写了一篇关于Landing AI团队在视觉提示方面所做工作的文章。随着通过文本提示和视觉提示构建机器学习应用程序的速度加快，我看到了不使用测试集来构建和部署模型的趋势。这是加速将模型投入生产的一个重要部分。<br><br>测试集一直是机器学习发展的一个神圣方面。在学术的机器学习工作中，测试集是算法基准测试和发布科学结论的基石。测试集也被用于商业机器学习应用程序，以判断和提高性能，并确保系统在部署前后的准确性。<br><br>归功于基于提示的开发——你可以简单通过提供文本提示（例如“将以下文本按具有积极或消极情绪分类”）或视觉提示（通过标记少量像素向模型传达你想要分类的对象）来构建模型——我们有可能构建一个具有很少样本（少样本学习）或根本没有样本（零样本学习）的像样的机器学习模型。<br><br>此前，如果我们需要10,000个标记过的训练样本，那么收集1,000个测试样本所产生的额外成本似乎并不繁重。但是，在基于提示开发的驱动下，零样本学习和少样本学习的兴起使测试集的收集成为一个瓶颈。<br><br>因此，我看到越来越多的团队使用如下的开发和部署流程:<br>● 使用提示来开发模型。这可能需要花费几分钟到几个小时。<br>● 将模型部署到生产环境中，并在实时数据上快速而安全地运行它，这也许可以在“影子模式”下运行。在这种模式下，模型的推理被存储和监视，但尚未被使用。(详见下文。)<br>● 如果模型的性能是可以接受的，就让它开始做出真正的决策。<br>● 只有在模型投入生产后，并且只有当我们需要更仔细地进行基准测试时（例如，尽力维持几个百分点的性能改进），才会为进一步的实验和开发创建更仔细的基准测试而收集测试数据。但如果系统运行得足够好，就不必为此费心了。<br><br>我对这个过程感到兴奋，它大大缩短了构建和部署机器学习模型所需的时间。然而，这里有一个重要的警示：在某些应用程序中，测试集对于管理危害风险很重要。许多部署不会造成重大伤害风险；例如，智能手机工厂的视觉检查系统最初会配备一名人工检查员（其检查结果没有被直接使用）。但是，如果我们开发的系统将涉及医疗保健、刑事司法、金融、保险等决策，那么不准确的输出或偏见可能会造成重大伤害。所以在允许模型做出相应决策之前，收集严格的测试集并深入验证模型的性能仍至关重要。<br><br>概念漂移和数据漂移的出现会使“测试集”这个概念在实际应用中出现问题，因为保存用于测试的数据不再与输入数据的真实分布相匹配。因此，最好的测试数据是生产数据。对于不使用测试集就可以安全合理地部署的应用程序，我很高兴看到它加快了机器学习应用程序的开发和部署。<br><br>请不断学习!<br>吴恩达</p></div></body></html>