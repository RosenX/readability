    <html>
      <body>
        <div><h1>吴恩达来信：更“省”数据的大型预训练模型</h1><p><i>Dear friends,</i></p><p><i>It’s time to move beyond the stereotype that machine learning systems need a lot of data. While having more data is helpful, large pretrained models make it practical to build viable systems using a very small labeled training set — perhaps just a handful of examples specific to your application.</i></p><p><i>About 10 years ago, with the rise of deep learning, I was one of the leading advocates for scaling up data and compute to drive progress. That recipe has carried us far, and it continues to drive progress in large language models, which are based on transformers. A similar recipe is emerging in computer vision based on large vision transformers.</i></p><p><i>But once those models are pretrained, it takes very little data to adapt them for a new task. With self-supervised learning, pretraining can happen on unlabeled data. So, technically, the model did need a lot of data for training, but that was unlabeled, general text or image data. Then, even with only a small amount of labeled, task-specific data, you can get good performance.</i></p><p><i>For example, say you have a transformer trained on a massive amount of text, and you want it to perform sentiment classification on your own dataset. The most common techniques are:</i></p><ul><li><i>Fine-tuning the model to your dataset. Depending on your application, this can be done with dozens or even fewer examples.</i></li><li><i>Few-shot learning. In this approach, you create a prompt that includes a few examples (that is, you write a text prompt that lists a handful of pieces of text and their sentiment labels). A common technique for this is</i> <i><a href="https://link.zhihu.com/?target=https%3A//ai.stanford.edu/blog/understanding-incontext/%3Futm_campaign%3DThe%2520Batch%26utm_source%3Dhs_email%26utm_medium%3Demail%26_hsenc%3Dp2ANqtz--R0fcwA-dwPxxE55xo0PMWk7Q65CeYDIhLEqkr6-fb5qmHwWNZdjGcdmGp9D19vxv3EBxB">in-context learning</a>.</i></li><li><i>Zero-shot learning, in which you write a prompt that describes the task you want done.</i></li></ul><p><i>These techniques work well. For example, customers of my team Landing AI have been building vision systems with dozens of labeled examples for years.</i></p><p><i>The 2010s were the decade of large supervised models, I think the 2020s are shaping up to be the decade of large pretrained models. However, there is one important caveat: This approach works well for unstructured data (text, vision and audio) but not for structured data, and the majority of machine learning applications today are built on structured data.</i></p><p><i>Models that have been pretrained on diverse unstructured data found on the web generalize to a variety of unstructured data tasks of the same input modality. This is because text/images/audio on the web have many similarities to whatever specific text/image/audio task you might want to solve. But structured data such as tabular data is much more heterogeneous. For instance, the</i> <i><a href="https://link.zhihu.com/?target=https%3A//www.kaggle.com/c/titanic%3Futm_campaign%3DThe%2520Batch%26utm_source%3Dhs_email%26utm_medium%3Demail%26_hsenc%3Dp2ANqtz--R0fcwA-dwPxxE55xo0PMWk7Q65CeYDIhLEqkr6-fb5qmHwWNZdjGcdmGp9D19vxv3EBxB">dataset of Titanic survivors</a></i> <i>probably has little in common with your company’s supply chain data.</i></p><p><i>Now that it's possible to build and deploy machine learning models with very few examples, it’s also increasingly possible to build and launch products very quickly — perhaps</i> <i><a href="https://link.zhihu.com/?target=https%3A//www.deeplearning.ai/the-batch/how-prompting-is-changing-machine-learning-development/%3Futm_campaign%3DThe%2520Batch%26utm_source%3Dhs_email%26utm_medium%3Demail%26_hsenc%3Dp2ANqtz--R0fcwA-dwPxxE55xo0PMWk7Q65CeYDIhLEqkr6-fb5qmHwWNZdjGcdmGp9D19vxv3EBxB">without even bothering to collect and use a test set</a>. This is an exciting shift. I’m confident that this will lead to many more exciting applications, including specifically ones where we don’t have much labeled data.</i></p><p><i>Keep learning!</i></p><p><i>Andrew</i></p><hr><p>亲爱的朋友们,<br><br>是时候抛开机器学习系统需要用到大量数据的刻板印象了。虽然拥有更多数据是有益的，但大型预训练模型能使利用非常小的标记训练集（可能只是针对你的应用程序的少量样本）构建可行系统变为现实。<br><br>大约10年前，随着深度学习的兴起，我也曾是扩大数据和计算规模以推动进步的主要倡导者之一。这种模式已经带我们走了很远，并且将继续推动基于transformer的大型语言模型的进步。基于大型视觉transformer的计算机视觉领域也出现了类似的模式。<br><br>然而，一旦这些模型被预先训练，只需要很少的数据就可以使它们适应新的任务。预训练可以通过自监督学习方式在未标记的数据上进行。（从技术上讲模型确实需要大量的数据来进行训练，但那是未标记的、一般的文本或图像数据。）如此，即便只掌握少量标记过的、特定于某些任务的数据，你也可以获得良好的性能。<br><br>假设你有一个在大量文本上训练的transformer，并且你希望它在你自己的数据集上执行情感分类。最常见的技巧是:<br>● 根据数据集对模型进行微调。对于你的应用程序，这可以用几十个甚至更少的样本来完成。<br>● 少样本 (Few-shot) 学习。在这种方法中，你需要创建一个包含几个样本的提示符（也就是说，编写一个文本提示符，列出一些文本片段及其情感标签）。一种常见的方法是上下文学习。<br>● 零样本 (Zero-shot ) 学习，你需要编写一个提示，描述你想要完成的任务。<br><br>这些技巧的效果很好。例如，我带领的Landing AI团队的客户多年来一直在用几十个带标签的样本构建视觉系统。<br><br>如果说2010年-2020年是属于大型监督模型的十年，那么我认为2020年-2030年将成为大型预训练模型的十年。然而，这里有一个重要的警示：这种方法适用于非结构化数据（文本、视觉和音频），而不适用于结构化数据（很遗憾目前大多数机器学习应用程序都是基于结构化数据构建的）。<br><br>在网络上抓取的各种非结构化数据上进行预训练的模型可以泛化到具有相同输入模式的各种非结构化数据任务。这是因为网络上的文本、图像、音频与你想要解决的特定文本、图像、音频任务有许多相似之处。但是结构化数据（如表格数据）的异构性要高得多。例如，泰坦尼克号幸存者的数据集可能与贵公司的供应链数据几乎没有共同之处。<br><br>现在，用很少的样本就可以构建和部署机器学习模型，快速构建和发布产品也越来越有希望——甚至可能不需要收集和使用测试集。这是一个令人兴奋的转变。我相信这将带来更多令人兴奋的应用，包括那些我们没有太多标记数据可用的应用。</p><p>请不断学习！</p><p>吴恩达</p></div>
      </body>
    </html>
    