    <html>
      <body>
        <div><h1>迈向 ChatGPT 时代 - 技术篇</h1><p>最近关于 AIGC，ChatGPT 等方面的消息和文章非常多，无论是从没了解过机器学习的圈外人士，还是每天跟模型打交道的专业从业者，都无不被 ChatGPT 的能力震惊。几年后来回顾这个事件，应该会成为通向通用人工智能/技术奇点的一个关键里程碑。这两个月我也不能免俗刷了很多相关文章和视频，过程中发现了不少颠覆我原有认知的信息。这篇文章主要是站在“巨人们的肩膀上”，从各个角度来聊聊我的收获和一些思考。</p><p>这也是我第一次尝试付费文章的形式，看看接受度如何 :) 本文的内容大致会分成技术，商业化和提升个人生产力三个角度来展开，每个部分主要内容都是个人的 key takeaways 和思考，同时把我目前看到的比较好的参考文章列出来。非常欢迎有不同想法的同学来一起探讨。从受众角度，应该比较适合：</p><ol><li>AI 领域从业者，想了解 LLM 和 AGI 的最新进展以及对自己工作的影响。但不太适合那种已经看了几十篇 LLM 领域论文的专业大佬，那样的话我这边写的内容应该你都很熟悉了。</li><li>平时用 Google，知乎等网站比较多的知识型工作者（尤其是软件工程师），对于 <a href="https://zhuanlan.zhihu.com/p/366187306">生产力工具</a> 非常感兴趣的同学，这篇文章应该也会比较合你胃口。</li><li>有创业或者相关领域投资方面想法的同学，但这方面的不确定性很强，个人见解仅供参考。</li><li>想了解通用人工智能对我们的工作，学习，生活的各种影响的其他同学，如果你不缺这几块钱的话 :)</li></ol><p>好了，话不多说，先让我们来看下 ChatGPT 背后的神奇技术与 OpenAI 的远大 vision。</p><h2>技术</h2><p>这部分先推荐一篇来自张俊林老师的文章 <a href="https://zhuanlan.zhihu.com/p/597586623">通向 AGI 之路：大型语言模型（LLM）技术精要</a>。一般来说，张老师如果在某个领域写了篇文章，在很长一段时间里我都不会考虑单独写这个话题了，贡献非常有限 。这部分会快速列举一些我之前不知道的或者令人意外的信息和思考角度。</p><h3>新范式：生成一切</h3><p>如果去读一些经典的机器学习教材，我们会发现大多数的算法建模任务都是以“输入信息，输出判别”的形式来思考的。例如给定一句话，希望模型来判断这句话表达的是正面还是负面的情感，这就是一个经典的分类问题。而在 NLP 中，又有很多“生成”性质的问题。例如提一个问题，希望模型给出答案。这种任务上的多样性，导致以往我们需要针对每一个任务来分别设计数据收集，模型训练流程，很难进行复用。当年 BERT 模型推出时，令我印象最深刻的也是它对于多任务统一的巧妙设计。</p><p>但到了以 GPT 为代表的大语言模型阶段，一个显著的思维方式变化是我们其实可以通过“生成”来解决一切问题。例如还是上面的情感分类问题，我们可以直接把语句输入给模型，并提问“这句话表达的是正面还是负面的情绪”，然后让模型对于上面的一整串输入，去生成一个输出回答。这样无论是分类，回归，还是生成类任务，都可以统一到一种框架下来实现了，也就可以使用统一的预训练模型来处理，非常优雅。这个生成一切的思维转变，也影响到了后面我们会提到的从 fine tune 思路到 prompt engineering 的思维转变等。</p><p>回过头来看，生成式的范式的确是很自然的想法，但在大多数领域我们目前都还做不到这点。比如我们能否输入一连串历史天气信息，让模型自动去生成未来几天的天气预报；输入用户的历史浏览、购物信息，生成出可能感兴趣的商品；输入一张图片，生成出图片中的物体名称及位置信息等等。如果这些任务都能以生成式的方式去做，或许未来能进一步形成多模态的预训练模型，解决更多领域的实际问题。</p><h3>自监督</h3><p>GPT 背后的模型原理出奇的简单，就是通过一系列输入文本，去预测后面会出现的文本是什么。大佬 Andrej Karpathy 在这个视频里 <a href="https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1E14y1M75n/">手把手教你如何从头实现 GPT</a>，如果对实现细节感兴趣的同学可以参考。这种自监督的模式使得我们能以较低成本获取海量的训练数据，例如 Wikipedia，出版书籍，新闻，各种公开 blog，Stack Overflow，GitHub 等等。而前面提到的一些其他类型的如图像/视频理解之类的问题，我们就比较难获取类似数量和质量的训练数据了。</p><p>当然也正是由于 GPT 背后的思想如此的简单，很多人（包括我在内）在一开始都觉得深度学习的本质并没有什么突破，仍然是在一堆数据中去拟合一个概率分布，后续在“生成”时做个采样而已。像各种知识概念，逻辑推理等，都是无法从当前的技术路线中产生的，需要融合像知识图谱，符号推理等手段才行。但在用过 ChatGPT 之后，相信大多数人都开始觉得这种简单的自回归模型或许真的能够带我们在通用人工智能的这条路上走得很远。</p><h3>遵循自然语言指令</h3><p>沿着生成一切的思路，一个自然的想法就是让模型能够“听懂”人类的指令，并做出期望的反馈回答。OpenAI 和 Google 都在这个方向上做了不少研究工作，略过技术细节，这些研究的目标是能够达到：</p><ul><li>让模型理解用户的意图是什么，而不只是简简单单做文本的“续写”输出。</li><li>对于模型没有见过的问题，也能很好地理解与回答（zero-shot）。</li><li>符合人类的价值观，例如输出的内容中应该遵循事实，语气友好，减少“有害”内容等。这一点 ChatGPT 做得格外好，很多人都觉得这是一个可以让自己孩子与之交谈学习的聊天机器人。很多其它大厂产品在这块都受到了很大的挑战。</li></ul><p>具体可以参考 <a href="https://link.zhihu.com/?target=https%3A//openai.com/blog/instruction-following/%23moon">OpenAI 这篇文章中的一些例子</a>。</p><p><br></p><figure><img src="https://pic4.zhimg.com/v2-d3b1b81f60c3fdbf4267c2d6e0d95ca7_b.jpg"></figure><p><br></p><p>打一个不太精确的比方，只用自监督训练形成的大语言模型相当于把互联网上所有的高质量文本信息都看了一遍，拥有了大量的知识，并且可以模仿人类的方式来续写文本，但它从来没有跟真实的人交互过，因此也无法理解人类的交互意图。通过人类指令和期望回答方面的学习，模型在“续写”能力上增加了“理解力”，从而成为一个真正可用的多任务通用模型。</p><h3>强化学习</h3><p>在做上述的人类指令学习方面，InstructGPT/ChatGPT 引入了人工标注与强化学习技术（RLHF）。从技术细节上来看，有几个比较有意思的点可以讨论。</p><p>如果让你来做人类指令的学习，你会怎么实现呢？一个最直观的思路肯定是给定一些问题，然后由人工来撰写回答，将这部分数据作为监督学习的样本放到模型中去训练。从 ChatGPT 的模型使用方式来看，也并不是像下棋一样跟人对弈，没有像典型的强化学习那样有“根据环境改变策略”的操作。所以很多人应该跟我一样，都觉得这里其实不用强化学习也可以达到类似的效果。</p><p>但后来仔细看了一下他们做 RLHF 的过程，发现有几个相对于直接做有监督 tuning 的提升的可能出发点：</p><ul><li>直接写问题的答案，相比来说对于人工标记员的水平要求和时间开销可能更高，学习到的指令理解可能也更偏向于标记人员的取向。而如果让标记员对模型生成的若干个回答做从好到坏的排序相对来说更容易一些，形成的 reward model 有更高的复用性，大大提升效率。</li><li>对于排序信息的利用也有一些小 trick，相当于做了一定的数据增强。这部分论文里有很详细的描述，包括怎么招标记团队等，几乎可以直接拿来使用。</li><li>他们在强化学习的 loss 中做了一些特殊设计，使得模型能符合人类预期，同时在 in context learning 和模型任务上不会损失太多性能。不过这一点应该在有监督中也可以做到，不知道具体效果有多大差别。</li><li>强化学习的流程更符合产品人机交互的形态，便于后续迭代。例如当前 ChatGPT 输出的回答，我们是可以提供正面或者负面反馈的，这些信息就相当于在给模型输出做排序，后面可以继续用在同样的框架里优化模型。这一点对于产品化来说非常重要，当前一亿的用户量能够提供的反馈数据量绝对是一大产品壁垒。</li><li>强化学习作为通用范式，未来有更大的想象空间，把在时间维度上做连续决策的能力也用上。例如聊天过程中为了更好地达成用户满意度（可以灵活选择 objective）的目标，可以先主动询问和澄清问题，再去做回答；模型生成的代码也可以在真实环境去运行，生成 reward signal，继续优化 reward 模型和 policy 模型。这与 OpenAI 发展通用人工智能的设想是高度一致的。</li></ul><p>所以总结来看，这里并不只是为了炫技而使用强化学习，而是有其背后深层的设计考量的。从自回归的“模仿学习”再到与真实环境交互的“强化学习”，或许就是 OpenAI 对于 AGI 的整体路线设计。</p><h3>从“微调”到“提示词”优化</h3><p>前面提到的让模型遵循自然语言指令，极大地改变了我们使用模型的方式。以往的常见思路是我们一般会有一个基于公开数据训练的模型，然后针对于特定领域的任务，我们再收集一波“领域专用”数据，然后做一下模型微调（fine tune）。就有点像我们有了一个各方面能力还过得去的普通人，然后训练它一下，让它更擅长做某种具体的任务（医学诊断，气象云图分析，自动驾驶之类）。这个思路看起来很美好，但也会碰到很多挑战：</p><ul><li>领域的新数据持续产生，需要持续的微调与部署，有较大计算成本。</li><li>微调本身改变了模型参数，可能导致“灾难性遗忘”。事实上 InstructGPT 就在提升理解能力的基础上，损失了一些任务的性能，被称为“对齐税”。</li><li>过程中模型性能会持续下降，等到下一次微调再提升回来，有一定的滞后性。</li><li>微调数据可能与原有训练数据的分布差别较大，导致模型的泛化性降低。这也不太像人类的工作方式 :)</li></ul><p>所以在 OpenAI 的一系列工作中，都非常突出 zero-shot/few-shot 方向的能力优化。简单来说，就是在碰到一个新领域的问题时，我是否可以不微调模型，而是改变一下我问问题的方式，就能让模型做出比较好的回答。Zero-shot，指的是我完全不给任何任务相关的例子，就希望模型来回答，有时候也被称为 prompt/instruct 的设计/调优。Few-shot，则是我在问题中给出几个例子，然后再让模型来回答，有时候也被称为 in context learning。但不管怎么说，这个思路都是令人震撼的，相当于我没有改变任何模型的参数，仅仅通过不同的提问方式，就能让它知道应该怎么回答。</p><h3>“问一个好的问题”</h3><p>知道答案不重要，能问出好的问题才是核心能力。这句话放在当前的大语言模型上真是再合适不过了。这里有非常多神奇的现象，我们分别来看一下。</p><h3>Chain of Thought</h3><p>人们在使用 GPT 这类模型中发现，一个原始的问题，模型可能没法正确回答，但是只要在提问中加一句“让我们一步一步来思考”，或者对于给出样例的问题中，同时给出一些推理过程，那么模型就能大幅度提升回答的准确率。第一次看到这个发现，我相信很多人脑海里都会出现一堆“黑人问号”的表情……看起来模型已经拥有了强大的推理能力。</p><h3>Few-shot Prompt</h3><p>在一些模型没有见过的任务上，我们可以给出一些样例，比如我们在做新产品的头脑风暴，可以描述一下大概想要的几种创意，然后让模型来生成更多。这里比较有意思的在于，假设我们举的例子是一系列 f(x) = y 的数据对，这个 y 的值是否准确并不重要，而只要 y 的总体分布符合真实分布就可以。这跟两个真实的人交谈很像，“我举个例子”，这个例子只要形式上正确就行，具体数值是否精确并不妨碍另外一个人对 context 的理解。</p><p>这一点可以说与传统的 fine tune 的思维发生了巨大的转变，我们现在完全可以向一个固定的模型去提问，在问题中包含一些“训练样本”，模型就能在不改变 weights 的情况下，实时学习到这些内容，并产生我们想要的输出。简单来说就是，amazing！</p><p>从技术角度分析，这篇 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2212.10559">Why Can GPT Learn In-Context?</a> 文章给出了一个很有意思的解释，in context learning 可以等价于在做一个少量样本的 fine tune。</p><h3>复杂的组合应用</h3><p>我们可以进一步把问问题串联起来，完成复杂任务。比如：</p><ul><li>可以给模型一些样例，让它来生成问题指令（soft prompting），然后再以这个问题去问模型。</li><li>可以对一个问题提供多种思路，分别来问模型，最后再集成起来让模型自己做个总结之类。</li><li>可以把模型的回答再输入给模型，问它“你觉得上面的回答是否不够友好/含有歧视，是否可以改进”等，进而提升模型的安全性。</li><li>可以把一个问题拆解成多个子问题，一个一个问，然后再把问题和回答作为 context，组合起来问最终的问题，实现分而治之。</li><li>还有像 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2212.10560">Self-Instruct</a> 这样的应用，让模型自己来生成数据“训练”自己！</li></ul><p><br></p><figure><img src="https://pic2.zhimg.com/v2-4b8f9118dfc992439cfb11ad9e102615_b.jpg"></figure><p><br></p><h3>激发模型能力</h3><p>从上面这些例子可以看出，大语言模型拥有非常类似于人的思维能力，只是需要通过正确的指令来进行激发。就如前面所说，模型已经学习了互联网上的所有文本，具有了非常丰富的知识和潜在的推理能力（后续会详细解释）。但这个模型是没有与真实世界的感知连接的。比如真实的两个人在交谈时，现在的时间，地点，环境等等情境信息，我们都是可以通过真实的感知获得的，因此在对话中双方都有这部分的预设信息。而对于模型来说，简单的一个问题可能并没有办法让它理解当前的情境和提问者的意图，是想让我做创意想象，还是做逻辑推理，还是查询事实知识等。这些信息都依赖于指令本身来提供，而好的指令能够激发模型，选择其相应的“记忆”与“能力”，完成具体任务。</p><p>更通俗来说，即使是同一件事情，我们在做工作汇报时的思维与表达方式，和跟同事吃饭时闲聊的描述的方式，都会有很大的不同。对于模型来说更是如此。因此详细的问题情境描述能够把让模型把特定的“思考方式”调用出来，实现能力的激发。这也是神奇的 in context learning 的一种直观理解方式。</p><h3>ChatGPT 的能力从何而来</h3><p>对于上述提到的 ChatGPT 的神奇能力，大家应该都很好奇到底是从何而来。好像不需要知识图谱，符号主义的那套方法，模型也能记住各种知识和它们之间的关联，并做一些简单的逻辑推理。这里有一篇非常好的文章，推断了 <a href="https://link.zhihu.com/?target=https%3A//yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1%23c45fd4df87a349028ade2d9dbeb1188f">GPT 系列模型中各种能力的来源</a>。</p><p><br></p><figure><img src="https://pic2.zhimg.com/v2-2551c51bd2bd3f58ee70da59b9199901_b.jpg"></figure><p><br></p><p>简单总结来说：</p><ul><li>大量文本数据（万亿级别 token 数量）做预训练：语言生成能力，基础的世界知识，in context learning 的基础。</li><li>足够大的模型规模（千亿级别参数）：大量知识的记忆与获取能力。</li><li>在代码数据上训练：复杂推理能力，长距离依赖关系的学习。这也是最令人着迷的一点。</li><li>有监督的指令微调：响应人类问题并泛化到没有见过的问题（zero-shot）上。减弱了 in context learning 的能力。目前从论文中看，指令微调和 RLHF 这块用到的打标数据量级并不大（十万条以内），相比自监督部分的数据量可谓是沧海一粟。</li><li>RLHF：总体让回答更加丰富具体，也增强了与人类期望的一致性（zero-shot，价值观，内容安全，避免胡说），但会降低模型在任务上的性能。比较有意思的是都用了 RLHF，text-003 模型也跟 ChatGPT 有不同的能力侧重，可惜目前没有论文透露其中的细节。</li></ul><p>如果在代码数据上训练能够获取到一定的推理能力，这里就能延展出一些其它想法，例如：</p><ul><li>是否在论文，数学类的教材文章中也能获取逻辑推理能力？</li><li>不同的编程语言对于推理能力的帮助是否会有不同，例如 Haskell，Coq 这类。</li><li>是否还有其它形式的数据，能够对增强推理能力很有帮助？</li></ul><h3>知识的获取与修改</h3><p>不过目前 ChatGPT 类模型也是有显著局限性的，比较突出的一点是大家在试用时经常发现的，它所学习的信息来源截止于 2021 年，因此后续的信息都无法获取准确的答案。虽然我们的确可以通过 instruct 去实时地“修改”一些知识，但这个信息并不会被“持久化”。在张俊林老师的文章中，提到了很多大语言模型是如何存储和获取知识的原理，也提到了未来如何去高效修改这些知识的一些方向探索。如果我们在模型使用上已经实现了自然语言接口，那么是否能进一步去实现模型训练的自然语言接口（instruct training）呢？这或许也会是迈向通用人工智能的重要一步，当然也需要有很多安全方面的考量。</p><h3>多模态</h3><p>迈向通用人工智能，另外一个比较显而易见的目标是将这类生成式范式应用到多模态上。比如去年另外一个很火的 AIGC 领域就是以 Stable Diffusion、Dall-E 2、Midjourney 等产品为代表的文生图应用。如果我们找到了多模态上的大模型之路，或许就能实现通过自然语言给 AI 一个描述，它就能自动生成出构造零件和组装的 3D 设计图，看到这个设计图，就能自动生成出流水线/机器人的一系列动作把真实的产品构造出来。甚至 AI 可以根据需求先构建机器人，再通过这些机器人去构建各种真实世界的产品，那样的生产成本大幅下降的未来会是何等景象真是难以想象。</p><p>像自动驾驶也是一个很典型的领域。如果仅仅从视觉来“生成”驾驶动作，难度是非常大的。而我们真实人类在开车过程中，一定是综合结合了视觉，声音，以及很多“内心独白”（语言形式的思考）作出相应的决策动作。整合多模态的输入信息会对实现相应的场景能力有很大的促进作用。</p><p>但结合我们前面提到的当前大语言模型的各种范式来说，多模态的数据很多都还没找到类似的实现路径，例如文本数据我们天然可以做用前文预测后一个单词的自监督训练，但用图片/视频生成文本这类数据就显然少了很多。此外不同模态是否能很好地融合在一个模型里也是个问题，直观上来看语言显然是一个很特别的存在，而图片视频这类的数据形态，分布，信息密度上会有很大的不同。所以当前多模态模型上的尝试，比如 ALBEF，VLMo 等都需要在多模态上做很多专门的模型结构设计。当然也有一些很有意思的发现，例如在 VLMo 和 BeiTv3 里都对多模态共享了注意力层，期待未来会有更多有意思的融合统一。</p><p>另外一个值得思考的点在于，各种模态对于形成“智能”的重要性如何，更多的模态是否会有促进作用。一个比较广泛的观点是语言还是人类智能中最为特殊的载体，动物类的智能也有用，但一般我们所说的通用人工智能指向的还是能够通过图灵测试的类人智能。而且即使是那些天生没有视觉、听觉的残障人士，其能够达到的智能水平也与普通人毫无差别，那是否说明其它模态的数据没有那么重要呢？还是说残障人士实际是通过触觉等补足了这方面的能力缺陷？知乎上的这篇 <a href="https://zhuanlan.zhihu.com/p/606364639">介绍 BLIP2 的文章</a> 中给出了一些很有趣的思路，把 LLM 作为一个非常核心的“处理器”，实现了非常惊艳的效果。后面在数据角度的讨论中我们也会再回到这个话题上。</p><h3>连接真实世界</h3><p>我们在使用 ChatGPT 时很容易碰到的一个局限是它有时候会生成出错误的回答，但却言之凿凿的样子，毕竟 ChatGPT 是一个被禁锢在服务器上的模型，目前唯一与真实世界的触点都来自与用户的对话交互和反馈，可谓是“不闻窗外事”。一个很自然的改进思路就是把模型与一些已有的外部系统连接起来，例如一些知识性的问题可以借助搜索引擎来佐证（后面会提到类似技术和产品），一些理工科相关的计算推理问题可以借助 Wolfram Alpha，生成的代码可以直接在真实环境编译执行并检验效果等。</p><p>这里有个通用的思考范式，以往我们需要去连接多个系统时，需要互相约定好比较特定的 API 格式。如果我们把语言模型当做一个输入是文本输出也是文本的“转换器”，那么这个转换器就可以与其它转换器很方便的去串联（回想一下前面提到的问题串联的玩法）。例如搜索引擎就是输入查询文本，返回网页文本的转换器，大语言模型也是输入文本（prompt），生成新的文本（回答），这两者就可以进行串联。例如用搜索引擎去搜索问题相关的文本信息，把文本信息作为背景，结合问题再给到语言模型去输出答案。甚至这个循环可以多做几步，问题 -&gt; 网页文本 -&gt; 更好的问题 -&gt; 更多的网页文本 -&gt; 多个问题同时输入到语言模型 -&gt; 再做 summary 等。未来可能就会在此基础上演化出很多领域特定的模型 pipeline 来更好的解决复杂问题，例如最近 Meta 的 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2302.04761">Toolformer</a> 采用的就是类似思路。</p><p><br></p><figure><img src="https://pic4.zhimg.com/v2-5772e843cff49e330fad918b9576f02b_b.jpg"></figure><p><br></p><p>如果从更长远来看，如果模型的输出能够直接形成外部系统的动作，后续又能将相关结果返回到模型，从而去迭代演化其知识与行为策略，那就非常接近强人工智能的终极形态了。当然我们是否要往那个方向发展是个问题，因为涉及真实世界的决策就会有更多安全方面的诉求考量。当前模型在这块的保障还很少，例如经常可以看到例子，通过一些指令引导，可以让模型给出危险的回答来。如果往那个方向发展，AI 的 reward 函数应该如何设计，是否与意识的产生相关，也都是需要深入思考的。</p><p>结合多模态与真实世界决策问题，已经有类似的研究项目再往这个方向前进了，例如 <a href="https://link.zhihu.com/?target=https%3A//minedojo.org/">MineDojo</a> 项目，搜集了各种 Minecraft 游戏相关的 YouTube 视频，Wiki 文章，Reddit 讨论，期望能训练一个模型能通过这些数据来学会如何在一个开放游戏环境中完成各种探索与任务。</p><p><br></p><figure><img src="https://pic2.zhimg.com/v2-2f916cc61613b68e50b43898be17e881_b.jpg"></figure><p><br></p><h3>思考“大力出奇迹”</h3><p>回到 GPT 这类大语言模型的诞生，我跟很多人一样，早期只是觉得这是一个大力出奇迹的产物，堆了很多数据，算力，训练出了巨大的模型，但好像没有什么特别的创新之处。但现在来看，这个结论显然是不够深入的，从前面一系列关于 AGI 路线的设想也能略窥一斑。之前在群里讨论时也有朋友提出，如果说数据和算力是最重要的因素，那么为何不是拥有最多数据和算力的大公司们，或者拥有产出最多算力基础设施的显卡厂商们搞出了 ChatGPT，而是这样一家小小的创业公司呢？</p><p>再深入去看这些研究人员在设计训练策略时的做法，也有很多有趣的发现。例如数据量，模型参数量和训练轮次是决定计算资源需求的几个要素，那么如何进行配比能让模型在最少资源需求的情况下达到最好的效果呢？是更多的数据更重要，还是更大的模型更重要，还是训练更多个 epoch 更重要？目前看可能最重要的还是高质量更大量的数据，并且与模型参数等形成一个比例关系来一起缩放，也就是所谓的 scaling law。</p><p>这类大模型还有一个比较有意思的现象，就是复杂系统中经常提到的“涌现”。在模型规模较小时，发现其在很多任务上的表现基本跟瞎猜差不多，但等到模型规模突破一个阈值之后，其相关能力会大幅度地上升，突然就可以完成这类任务了。这里面一个可能的关键因素就是之前提到的 chain of thought 能力，在复杂任务的各个环节的理解能力都逐渐达到了一个水平后，整个链路才得以打通，就突然可以做复杂的推理问题（in context learning）之类了。另外人类知识体系的复杂性也导致了必须掌握大部分的知识，才能去理解一个非常细小的行为。</p><p>过去几年我的很多思路都更倾向于 data-centric AI 里那种用少量高质量的数据去构建和完善特定领域模型，大语言模型在这一点上的特质的确让我打开了视野。如果人类可以识别和处理脏数据，那么足够复杂的大模型是否也能自动处理呢？</p><p>最后，很多人会觉得巨大的模型训练，推理成本会让很多实际应用因为成本问题无法落地。这一点我倒是非常乐观，从工业革命开始技术和计算能力方面的摩尔定律基本就没有失效过。因为担心算力成本而放弃产品化探索，或者过多地把技术壁垒建立在硬件资源的节约上可能反而比较有风险。当然如果本身就是做底层基础设施的厂商，能够契合这种“指数增长”趋势的技术肯定就是巨大的优势了。这方面像稀疏化的大模型，结合 retrieval 技术等都是很有意思的研究发展方向。</p><h3>数据用尽？</h3><p>前面已经提到，当前阶段影响大模型能力的最重要的因素就是高质量的数据。目前很多语言模型已经用了整个因特网上能够获取到的 10%量级的数据了，在可见的将来很可能会出现“数据用尽”的问题。对于这一点，个人有几个方面的思考。</p><p>首先数据的重要性很可能在一段时间内都会成为相关产品竞争之间的一个重要差异化因素（模型本身反而比较类似），因此在设计相关产品时，相应的“数据飞轮”设计需要重点考虑。这个方面也许会涌现出一些相关技术和厂商的机会来，例如专职于做人工/自动化数据生成，人类反馈数据收集的相关技术栈等。</p><p>对于目前 ChatGPT 生成的回复信息，很多也是全新的，从未出现过的文字组合。对于这种输出，是否是一种有效的“训练数据”呢？如果不是的话，那么有人类参与的“有效训练数据”与其本质区别是什么？这也类似于问模型目前给出的回答，是不是一种“创新”？</p><p>上述问题或许有些哲学意味，换一种说法，我们可以把新的信息的生成分为两类，一类是含义/事实/经验型的数据，相当于世界上发生的各种新事件。这类数据只需要以类似搜索引擎更新数据库的方式去更新模型应该就可以（现在的 prompt 也能解决一部分）。另一类则是逻辑/系统类的数据，体现的是信息之间关系的演变，例如出现了新的系统或组织，实体间的关联关系等。但后者这类信息的更新速度仅由人类来推动的话，或许是非常缓慢的。这也是为何有“太阳底下无新事”的说法存在。这种类型的信息更新，或许才是我们本质上需要去提供相关数据，增量训练模型的出发点。我们可以看到像 WebGPT、Sparrow 这类工作，以及 perplexity.ai、新版的 Bing 等产品都开始体现了 LLM 能够利用实时检索信息的能力，而不是任何的新知识都需要训练模型更新参数。</p><p>另一个更加实际的思考也是往多模态方向演进，思考未被“文本化”的知识主要在哪。例如人类日常中有非常多的语言文字交流是以口头形式完成的，那么各种面试，会议，播客访谈，日常聊天，甚至自言自语等是否都可以记录下来，形成更多更丰富的文本资料？例如像 <a href="https://link.zhihu.com/?target=https%3A//www.rewind.ai/">rewind</a> 这样的产品就已经在做一些类似形式的尝试了，说不定会成为一种趋势。</p><p>如果大模型有了与真实世界更多的交互能力，那么新数据的生成或许也真的不再受限于人类产生数据的速度了。但这个方向也是有很大风险的。即使仅考虑文字输出，例如有些人可以通过控制一系列的 <a href="https://link.zhihu.com/?target=https%3A//github.com/yaroslav-n/tweetGPT">twitter bot</a>，生成特定的社交媒体信息，达到操控舆论导向的目的。前阵子 Stack Overflow 禁止使用 ChatGPT 插件来回答问题，也是担心其产生的错误回答影响社区整体的信息质量。如果模型能做更多类型的决策，其安全风险必然更大。</p><p>数据安全，隐私等方面也是一个绕不过去的话题。尤其是当前的大语言模型极度依赖于数据，谁控制了训练（包括 instruct tuning 等）数据，谁就控制了模型的“价值取向”。这在模型开始得到更广泛规模的使用时就显得极为重要。虽然前面也设想了一下是否能够实现通过指令来做实时增量训练，但显然模型安全方面是个需要先考虑解决的前提条件。目前也有一些工作在往这个方向发展，例如 <a href="https://link.zhihu.com/?target=https%3A//github.com/mit-han-lab/offsite-tuning">Offsite-Tuning</a> 等。</p><h3>可解释机器学习</h3><p>在大语言模型的框架下，许多之前的研究都可以重新来审视和思考，这里仅以可解释机器学习来举个例子。</p><p><br></p><figure><img src="https://pic3.zhimg.com/v2-2bfa8690e6d6ad1c37148b9c6198c37e_b.jpg"></figure><p><br></p><p>上图是当前主流的 xai 的思考框架，从人和模型两方的知识范围，工作原理等角度出发，通过不同的路径去扩大两者的交集，促进沟通理解。从某种程度上来说，instruct tuning 或许也是一种新的形成可解释机器学习的方式。通过自然语言指令和多轮对话，我们可以反复确认模型给出预测背后参考的信息，推演逻辑等，这就与我们理解真实人类的想法非常接近了。在这个话题上是否还需要去了解人工神经网络的底层原理和特性可能就不那么重要了。</p><h3>AGI 随想</h3><p>前面已经提到了很多跟通用人工智能相关的思考与遐想，例如从监督学习，到模仿学习再到强化学习的路线；通过自然语言接口，连接真实环境来达到“知行合一”的独立智能体；通过多模态的数据去增强语言模型的信息交互和“涌现”能力，实现逻辑推理，完成复杂任务等等。不过在强人工智能方向，一个绕不过去的话题是机器能否有意识，或者说我们是否要去创造意识？人类在思考与决策上的很多“缺陷”（如《思考，快与慢》中提到的那些），是不是也有不少是由意识和情感这些因素产生的。如果我们想构建一种与我们互补的“工作伙伴”，或许意识并不是必需的。但如果我们想要让这类智能体在宇宙中去延续人类文明，那么意识可能就比较关键了（还要防止自我毁灭）。</p><p>另外前面提到的一个设想，如果通用智能后续能够自行完成物品的生产建造，甚至连机器人也都是自动化建造出来的，那么整个社会的商品成本都会极大下降。如果我们把想象更进一步，ChatGPT 是否学习过自己的代码？它能否通过当前模型在世界上的反馈表现去自动更新这部分的代码并不断迭代，甚至为自己创造出更利于自我生存的“意识”来？这是否会跟人类修改基因一样，也涉及到“机器人伦理”问题？:)</p><hr><p>本系列的后续文章：</p><div><a href="https://zhuanlan.zhihu.com/p/607127757">字节：迈向 ChatGPT 时代 - 商业篇</a></div></div>
      </body>
    </html>
    