    <html>
      <body>
        <h1>How should AI systems behave, and who should decide?</h1><div><p>In pursuit of our mission, we’re committed to ensuring that access to, benefits from, and influence over AI and AGI are widespread. We believe there are at least three building blocks required in order to achieve these goals in the context of AI system behavior.<span><sup><a href="#fn-B">B</a></sup><span><span><span>[B]</span></span></span></span></p><div><p>In this post, we deliberately focus on this particular scope, and on where we are going in the near term. We are also pursuing an ongoing research agenda taking on these questions.<br></p></div><p><strong>1. Improve default behavior</strong>. We want as many users as possible to find our AI systems useful to them “out of the box” and to feel that our technology understands and respects their&nbsp;values.</p><p>Towards that end, we are investing in research and engineering to reduce both glaring and subtle biases in how ChatGPT responds to different inputs. In some cases ChatGPT currently refuses outputs that it shouldn’t, and in some cases, it doesn’t refuse when it should. We believe that improvement in both respects is&nbsp;possible.</p><p>Additionally, we have room for improvement in other dimensions of system behavior such as the system “making things up.” Feedback from users is invaluable for making these&nbsp;improvements.</p><p><strong>2. Define your AI’s values, within broad bounds</strong>. We believe that AI should be a useful tool for individual people, and thus customizable by each user up to limits defined by society. Therefore, we are developing an upgrade to ChatGPT to allow users to easily customize its&nbsp;behavior.</p><p>This will mean allowing system outputs that other people (ourselves included) may strongly disagree with. Striking the right balance here will be challenging–taking customization to the extreme would risk enabling&nbsp;<a href="https://openai.com/blog/forecasting-misuse/">malicious uses</a>&nbsp;of our technology and sycophantic AIs that mindlessly amplify people’s existing&nbsp;beliefs.</p><p>There will therefore always be some bounds on system behavior. The challenge is defining what those bounds are. If we try to make all of these determinations on our own, or if we try to develop a single, monolithic AI system, we will be failing in the commitment we make in our Charter to “avoid undue concentration of&nbsp;power.”</p><p><strong>3. Public input on defaults and hard bounds</strong>. One way to avoid undue concentration of power is to give people who use or are affected by systems like ChatGPT the ability to influence those systems’&nbsp;rules.</p><p>We believe that many decisions about our defaults and hard bounds should be made collectively, and while practical implementation is a challenge, we aim to include as many perspectives as possible. As a starting point, we’ve sought external input on our technology in the form of&nbsp;<a href="https://github.com/openai/dalle-2-preview/blob/main/system-card.md">red teaming</a>. We also recently began&nbsp;<a href="https://platform.openai.com/docs/chatgpt-education/educator-input">soliciting public input</a>&nbsp;on AI in education (one particularly important context in which our technology is being&nbsp;deployed).</p><p>We are in the early stages of piloting efforts to solicit public input on topics like system behavior, disclosure mechanisms (such as watermarking), and our deployment policies more broadly. We are also exploring partnerships with external organizations to conduct third-party audits of our safety and policy&nbsp;efforts.<br></p></div>
      </body>
    </html>
    