    <html>
      <body>
        <div><h1>Function calling and other API updates</h1><!--[--><div><p>We released <code>gpt-3.5-turbo</code> and <code>gpt-4</code> earlier this year, and in only a short few months, have seen <a href="https://openai.com/customer-stories">incredible applications</a> built by developers on top of these models.</p><p>Today, we’re following up with some exciting updates:</p><ul><li>new function calling capability in the Chat Completions API</li><li>updated and more steerable versions of <code>gpt-4</code> and <code>gpt-3.5-turbo</code></li><li>new 16k context version of <code>gpt-3.5-turbo</code> (vs the standard 4k version)</li><li>75% cost reduction on our state-of-the-art embeddings model</li><li>25% cost reduction on input tokens for <code>gpt-3.5-turbo</code></li><li>announcing the deprecation timeline for the <code>gpt-3.5-turbo-0301</code> and <code>gpt-4-0314</code> models</li></ul><p>All of these models come with the same data privacy and security guarantees we introduced on March 1 — customers own all outputs generated from their requests and their API data will not be used for training.<br></p></div><p><h2>Function calling</h2></p><div><p>Developers can now describe functions to <code>gpt-4-0613</code> and <code>gpt-3.5-turbo-0613</code>, and have the model intelligently choose to output a JSON object containing arguments to call those functions. This is a new way to more reliably connect GPT's capabilities with external tools and APIs.</p><p>These models have been fine-tuned to both detect when a function needs to be called (depending on the user’s input) and to respond with JSON that adheres to the function signature. Function calling allows developers to more reliably get structured data back from the model. For example, developers can:</p><ul><li>Create chatbots that answer questions by calling external tools (e.g., like ChatGPT Plugins)</li></ul><p>Convert queries such as “Email Anya to see if she wants to get coffee next Friday” to a function call like <code>send_email(to: string, body: string)</code>, or “What’s the weather like in Boston?” to <code>get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')</code>.</p><ul><li>Convert natural language into API calls or database queries</li></ul><p>Convert “Who are my top ten customers this month?” to an internal API call such as <code>get_customers_by_revenue(start_date: string, end_date: string, limit: int)</code>, or “How many orders did Acme, Inc. place last month?” to a SQL query using <code>sql_query(query: string)</code>.</p><ul><li>Extract structured data from text</li></ul><p>Define a function called <code>extract_people_data(people: [{name: string, birthday: string, location: string}])</code>, to extract all people mentioned in a Wikipedia article.</p><p>These use cases are enabled by new API parameters in our <code>/v1/chat/completions</code> endpoint, <code>functions</code> and <code>function_call</code>, that allow developers to describe functions to the model via JSON Schema, and optionally ask it to call a specific function. Get started with our <a href="https://platform.openai.com/docs/guides/gpt/function-calling">developer documentation</a> and <a href="https://github.com/openai/evals">add evals</a> if you find cases where function calling could be improved<br></p></div><div><section><div><p><h1>Function calling example</h1></p><div><div><p>What’s the weather like in Boston right now?</p></div></div><!--[--><div><div><p><span>Step 1</span><span>·</span><span>OpenAI API</span></p><span>Call the model with functions and the user’s input</span></div><div><div><ul><!--[--><!--]--></ul></div><div><pre><code>curl https://api.openai.com/v1/chat/completions -u :$OPENAI_API_KEY -H 'Content-Type: application/json' -d '{
  "model": "gpt-3.5-turbo-0613",
  "messages": [
    {"role": "user", "content": "What is the weather like in Boston?"}
  ],
  "functions": [
    {
      "name": "get_current_weather",
      "description": "Get the current weather in a given location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA"
          },
          "unit": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"]
          }
        },
        "required": ["location"]
      }
    }
  ]
}'</code></pre></div><div><pre><code>{
  "id": "chatcmpl-123",
  ...
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": null,
      "function_call": {
        "name": "get_current_weather",
        "arguments": "{ \"location\": \"Boston, MA\"}"
      }
    },
    "finish_reason": "function_call"
  }]
}</code></pre></div></div></div><div><div><p><span>Step 2</span><span>·</span><span>Third party API</span></p><span>Use the model response to call your API</span></div><div><div><ul><!--[--><!--]--></ul></div><div><pre><code>curl https://weatherapi.com/...</code></pre></div><div><pre><code>{ "temperature": 22, "unit": "celsius", "description": "Sunny" }</code></pre></div></div></div><div><div><p><span>Step 3</span><span>·</span><span>OpenAI API</span></p><span>Send the response back to the model to summarize</span></div><div><div><ul><!--[--><!--]--></ul></div><div><pre><code>curl https://api.openai.com/v1/chat/completions -u :$OPENAI_API_KEY -H 'Content-Type: application/json' -d '{
  "model": "gpt-3.5-turbo-0613",
  "messages": [
    {"role": "user", "content": "What is the weather like in Boston?"},
    {"role": "assistant", "content": null, "function_call": {"name": "get_current_weather", "arguments": "{ \"location\": \"Boston, MA\"}"}},
    {"role": "function", "name": "get_current_weather", "content": "{\"temperature\": "22", \"unit\": \"celsius\", \"description\": \"Sunny\"}"}
  ],
  "functions": [
    {
      "name": "get_current_weather",
      "description": "Get the current weather in a given location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA"
          },
          "unit": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"]
          }
        },
        "required": ["location"]
      }
    }
  ]
}'
</code></pre></div><div><pre><code>{
  "id": "chatcmpl-123",
  ...
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "The weather in Boston is currently sunny with a temperature of 22 degrees Celsius.",
    },
    "finish_reason": "stop"
  }]
}
</code></pre></div></div></div><!--]--><div><div><!--[--><span>OAI</span><!--]--></div><p> The weather in Boston is currently sunny with a temperature of 22 degrees Celsius. </p></div></div></section></div><p>Since the alpha release of ChatGPT plugins, we have learned much about making tools and language models work together safely. However, there are still open research questions. For example, a proof-of-concept exploit illustrates how untrusted data from a tool’s output can instruct the model to perform unintended actions. We are working to mitigate these and other risks. Developers can protect their applications by only consuming information from trusted tools and by including user confirmation steps before performing actions with real-world impact, such as sending an email, posting online, or making a purchase.<br></p><p><h2>New models</h2></p><p><h3>GPT-4</h3></p><div><p><code>gpt-4-0613</code> includes an updated and improved model with function calling.</p><p><code>gpt-4-32k-0613</code> includes the same improvements as <code>gpt-4-0613</code>, along with an extended context length for better comprehension of larger texts.</p><p>With these updates, we’ll be inviting many more people from <a href="https://openai.com/waitlist/gpt-4-api">the waitlist</a> to try GPT-4 over the coming weeks, with the intent to remove the waitlist entirely with this model. Thank you to everyone who has been patiently waiting, we are excited to see what you build with GPT-4!<br></p></div><p><h3>GPT-3.5 Turbo</h3></p><div><p><code>gpt-3.5-turbo-0613</code> includes the same function calling as GPT-4 as well as more reliable steerability via the system message, two features that allow developers to guide the model's responses more effectively.</p><p><code>gpt-3.5-turbo-16k</code> offers 4 times the context length of <code>gpt-3.5-turbo</code> at twice the price: $0.003 per 1K input tokens and $0.004 per 1K output tokens. 16k context means the model can now support ~20 pages of text in a single request.<br></p></div><p><h3>Model deprecations</h3></p><div><p>Today, we’ll begin the upgrade and deprecation process for the initial versions of <code>gpt-4</code> and <code>gpt-3.5-turbo</code> that we <a href="https://openai.com/blog/introducing-chatgpt-and-whisper-apis#:~:text=Chat%20guide.-,ChatGPT%20upgrades,-We%20are%20constantly">announced in March</a>. Applications using the stable model names (<code>gpt-3.5-turbo</code>, <code>gpt-4</code>, and <code>gpt-4-32k</code>) will automatically be upgraded to the new models listed above on June 27th. For comparing model performance between versions, our <a href="https://github.com/openai/evals">Evals library</a> supports public and private evals to show how model changes will impact your use cases.&nbsp;</p><p>Developers who need more time to transition can continue using the older models by specifying <code>gpt-3.5-turbo-0301</code>, <code>gpt-4-0314</code>, or <code>gpt-4-32k-0314</code> in the ‘model’ parameter of their API request. These older models will be accessible through September 13th, after which requests specifying those model names will fail. You can stay up to date on model deprecations via our <a href="https://platform.openai.com/docs/deprecations/">model deprecation page</a>. This is the first update to these models; so, we eagerly welcome <a href="https://community.openai.com/">developer feedback</a> to help us ensure a smooth transition.<br></p></div><p><h2>Lower pricing</h2></p><p>We continue to make our systems more efficient and are passing those savings on to developers, effective today.<br></p><p><h3>Embeddings</h3></p><p><code>text-embedding-ada-002</code> is our most popular embeddings model. Today we’re reducing the cost by 75% to $0.0001 per 1K tokens.<br></p><p><h3>GPT-3.5 Turbo</h3></p><div><p><code>gpt-3.5-turbo</code> is our most popular chat model and powers ChatGPT for millions of users. Today we're reducing the cost of <code>gpt-3.5-turbo</code>’s input tokens by 25%. Developers can now use this model for just $0.0015 per 1K input tokens and $0.002 per 1K output tokens, which equates to roughly 700 pages per dollar.</p><p><code>gpt-3.5-turbo-16k</code> will be priced at $0.003 per 1K input tokens and $0.004 per 1K output tokens.</p><p>Developer feedback is a cornerstone of our platform’s evolution and we will continue to make improvements based on the suggestions we hear. We’re excited to see how developers use these latest models and new features in their applications.<br></p></div><!--]--></div>
      </body>
    </html>
    